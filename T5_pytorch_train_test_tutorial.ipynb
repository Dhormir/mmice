{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 training and evaluation with Pytorch + Accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\multilingual_mice\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(\"text-classification\",\n",
    "                 model=\"Yu-yang/bert-finetuned-20newsgroups\",\n",
    "                 tokenizer=\"bert-base-uncased\"\n",
    "                 )\n",
    "#model.save_pretrained(\"./trained_predictors/20newsgroups/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_42k_hcuch(data_files=None, column_names=['hallazgos', 'impresion', 'nodulos']):\n",
    "    kwargs = {\"encoding\" : \"utf-8\", \"sep\" : \"|\", \"dtype\": {'nodulos' : int}}\n",
    "    df_list = [pd.read_csv(data_file, **kwargs) for data_file in data_files]\n",
    "    df = pd.concat(df_list)[column_names]\n",
    "    df['text'] = df['hallazgos'] # + \" \" +  df['impresion']\n",
    "    \n",
    "    data = Dataset.from_pandas(df).remove_columns(['hallazgos', 'impresion', '__index_level_0__'])\n",
    "    data = data.rename_column('nodulos', 'label')\n",
    "    data = data.shuffle(42)\n",
    "    return data.train_test_split(train_size=.75)\n",
    "\n",
    "\n",
    "data_files = [\"data/42k_HCUCH\\labeled_data_3-label_test.csv\",\n",
    "              \"data/42k_HCUCH\\labeled_data_3-label_train.csv\",\n",
    "              \"data/42k_HCUCH/labeled_data_3-label_validation.csv\"]\n",
    "\n",
    "load_42k_hcuch(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "tokenizer.save_pretrained(\"./trained_predictors/chileanhate/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\multilingual_mice\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"trained_predictors/42k_HCUCH/models\",\n",
    "    tokenizer=\"xlm-roberta-large\",\n",
    "    # return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nodulos'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the model\n",
    "\n",
    "model_name = \"trained_predictors/42k_HCUCH/models\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Extract label mapping\n",
    "label_mapping = model.config.id2label  # Dictionary: {0: \"label1\", 1: \"label2\", ...}\n",
    "\n",
    "print(label_mapping)  # See available labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'nodulos', 'score': 0.04308806732296944}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"testing para nodulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, AutoConfig\n",
    "\n",
    "# Load the model\n",
    "model_name = \"trained_predictors/42k_HCUCH/models_I\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Specify your target label index (for example, 2)\n",
    "target_index = 1\n",
    "\n",
    "# Extract the weights and bias for the target label\n",
    "# The original classifier is typically an instance of nn.Linear with shape [num_labels, hidden_size]\n",
    "old_weights = model.classifier.out_proj.weight.data[target_index].unsqueeze(0)  # shape: [1, hidden_size]\n",
    "old_bias = model.classifier.out_proj.bias.data[target_index].unsqueeze(0)       # shape: [1]\n",
    "\n",
    "# Create a new linear layer with a single output\n",
    "new_classifier = nn.Linear(model.config.hidden_size, 1)\n",
    "\n",
    "# Set the weights and bias of the new classifier to the extracted values\n",
    "new_classifier.weight.data = old_weights\n",
    "new_classifier.bias.data = old_bias\n",
    "\n",
    "# Create a new linear layer for binary classification.\n",
    "new_classifier = nn.Linear(model.config.hidden_size, 1)\n",
    "\n",
    "# Initialize it with the extracted weights and bias.\n",
    "new_classifier.weight.data = old_weights\n",
    "new_classifier.bias.data = old_bias\n",
    "\n",
    "# Replace the projection layer with the new classifier.\n",
    "model.classifier.out_proj = new_classifier\n",
    "\n",
    "# Update the model configuration to indicate binary classification.\n",
    "model.config.num_labels = 1\n",
    "\n",
    "# Assuming 'model' is your modified model\n",
    "model.config.id2label = {0: \"nodulos\"}\n",
    "model.config.label2id = {\"nodulos\": 0}\n",
    "\n",
    "\n",
    "model.save_pretrained(\"trained_predictors/42k_HCUCH/models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check for cuda use\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text'],\n",
       "    num_rows: 11314\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =  load_dataset('SetFit/20_newsgroups', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'label_text'],\n",
       "     num_rows: 9362\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label', 'label_text'],\n",
       "     num_rows: 6231\n",
       " })}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a set of the selected label IDs for faster lookup\n",
    "selected_label_ids = set(range(16))\n",
    "\n",
    "# Define a filter function\n",
    "def filter_first_16_labels(example):\n",
    "    return example['label'] in selected_label_ids\n",
    "\n",
    "# Apply the filter to each split\n",
    "filtered_dataset = {\n",
    "    split: data.filter(filter_first_16_labels)\n",
    "    for split, data in dataset.items()\n",
    "}\n",
    "\n",
    "filtered_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9556f87572954ad2840b88b80c73f971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdef0a691ad645fc9fb7dc9464857d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_text(example, special_chars=[\"\\n\", \"\\t\", \"\\x85\", \"\\x97\", \"#\", \"<br />\", \"<br/>\"]):\n",
    "    text = example['text']\n",
    "    for char in special_chars:\n",
    "        if char in text:\n",
    "            text = text.replace(char, \" \")\n",
    "    example['text'] = text.encode().lower()\n",
    "    return example\n",
    "\n",
    "a = load_dataset(\"SetFit/20_newsgroups\", split='test').filter(lambda example:  example['label'] in set(range(16))).map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\multilingual_mice\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is an example of a longer sentence that we want to tokenize and truncate to a maximum length.\n",
      "Truncated text: this is an example of a longer sentence\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"This is an example of a longer sentence that we want to tokenize and truncate to a maximum length.\"\n",
    "\n",
    "# Define the maximum token length\n",
    "max_length = 10\n",
    "\n",
    "# Tokenize with truncation\n",
    "encoded = tokenizer(\n",
    "    input_text, \n",
    "    truncation=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Decode the truncated tokens back to a string\n",
    "truncated_text = tokenizer.decode(encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Original text:\", input_text)\n",
    "print(\"Truncated text:\", truncated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# rich: for a better display on terminal\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "#console = Console(record=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(source_text.split())\n",
    "        target_text = \" \".join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.summ_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze()\n",
    "        target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to evaluate model for predictions\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  actuals = []\n",
    "  with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          if _%10==0:\n",
    "              print(f'Completed {_}')\n",
    "\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "  return predictions, actuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation With BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, BertForMaskedLM, BertTokenizerFast, pipeline\n",
    "import random\n",
    "\n",
    "\n",
    "def predict_seqs_dict(sequence, model, tokenizer, top_k=5, order=\"right-to-left\"):\n",
    "\n",
    "    ids_main = tokenizer.encode(sequence, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    ids_ = ids_main.detach().clone()\n",
    "    position = torch.where(ids_main == tokenizer.mask_token_id)\n",
    "\n",
    "    positions_list = position[1].numpy().tolist()\n",
    "\n",
    "    if order == \"left-to-right\":\n",
    "        positions_list.reverse()\n",
    "\n",
    "    elif order == \"random\":\n",
    "        random.shuffle(positions_list)\n",
    "\n",
    "    # print(positions_list)\n",
    "    predictions_ids = {}\n",
    "    predictions_detokenized_sents = {}\n",
    "\n",
    "    for i in range(len(positions_list)):\n",
    "        predictions_ids[i] = []\n",
    "        predictions_detokenized_sents[i] = []\n",
    "\n",
    "        # if it was the first prediction,\n",
    "        # just go on and predict the first predictions\n",
    "\n",
    "        if i == 0:\n",
    "            model_logits = model(ids_main)[\"logits\"][0][positions_list[0]]\n",
    "            top_k_tokens = torch.topk(model_logits, top_k, dim=0).indices.tolist()\n",
    "\n",
    "            for j in range(len(top_k_tokens)):\n",
    "                # print(j)\n",
    "                ids_t_ = ids_.detach().clone()\n",
    "                ids_t_[0][positions_list[0]] = top_k_tokens[j]\n",
    "                predictions_ids[i].append(ids_t_)\n",
    "\n",
    "                pred = tokenizer.decode(ids_t_[0])\n",
    "                predictions_detokenized_sents[i].append(pred)\n",
    "\n",
    "                # append the sentences and ids of this masked token\n",
    "\n",
    "        # if we already have some predictions, go on and fill the rest of the masks\n",
    "        # by continuing the previous predictions\n",
    "        if i != 0:\n",
    "            for pred_ids in predictions_ids[i - 1]:\n",
    "\n",
    "                # get the logits\n",
    "                model_logits = model(pred_ids)[\"logits\"][0][positions_list[i]]\n",
    "                # get the top 5 of this prediction and masked token\n",
    "                top_k_tokens = torch.topk(model_logits, top_k, dim=0).indices.tolist()\n",
    "\n",
    "                for top_id in top_k_tokens:\n",
    "\n",
    "                    ids_t_i = pred_ids.detach().clone()\n",
    "                    ids_t_i[0][positions_list[i]] = top_id\n",
    "\n",
    "                    pred = tokenizer.decode(ids_t_i[0])\n",
    "\n",
    "                    # append the sentences and ids of this masked token\n",
    "\n",
    "                    predictions_ids[i].append(ids_t_i)\n",
    "                    predictions_detokenized_sents[i].append(pred)\n",
    "\n",
    "    return predictions_detokenized_sents\n",
    "\n",
    "\n",
    "sequence = \"This is some super neat [MASK] !\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#pipe = pipeline(task=\"fill-mask\", tokenizer=tokenizer, model=model)\n",
    "print(predict_seqs_dict(sequence, model, tokenizer))\n",
    "#print(pipe(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "model_id = \"bert-base-uncased\" # \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_id)\n",
    "model = BertForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "sequence = \" i've got as much [MASK] as the next b[MASK] e, and ra[MASK] [MASK] at her [MASK] is [MASK] [MASK] a [MASK] ; [MASK] the fact is that a [MASK] cut-out could act better, and an [MASK] [MASK] [MASK] of ms. w showing off her considerable assets [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] the cast, it's [MASK] that [MASK]'[MASK] [MASK] [MASK] [MASK] it is. i've never been a big fan of [MASK], and his tough guy harry is about as [MASK] as a 9 - dollar bill. godfrey cambridge and [MASK] de sica, both of whom i usually enjoy, seem to be [MASK] through their lines ; and as for edward g... well, i can only assume he was there for the paycheck [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] non - existent [MASK], through stop - start action and unfunny [MASK] to puerile slapstick and [MASK] 60's'caper'music [MASK] [MASK] [MASK] weren't for miss welch, i'd have given it [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] in'bedazzled [MASK] [MASK] [MASK] for that [MASK] alone i gave it a 3.\"\n",
    "\n",
    "start = time.time()\n",
    "results = predict_seqs_dict(sequence, model, tokenizer)\n",
    "end = time.time()\n",
    "#pipe = pipeline(task=\"fill-mask\", tokenizer=tokenizer, model=model)\n",
    "print(f'it took {end-start} seconds')\n",
    "print(results)\n",
    "#print(pipe(sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
