{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your desired cache directory\n",
    "os.environ['HF_HOME'] = 'D:\\Repositories\\multilingual_mice\\.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f31562a11d4a8ab78cbb6ec425b9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc7b93307f644d88f4edfcef53c0f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5TokenizerFast'.\n",
      "d:\\Repositories\\multilingual_mice\\.venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mmice.utils import html_highlight_diffs\n",
    "from mmice.edit_finder import EditEvaluator\n",
    "from mmice.maskers.random_masker import RandomMasker\n",
    "from transformers import MT5TokenizerFast\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "eval = EditEvaluator(fluency_model_name=\"google/mt5-small\",\n",
    "                     fluency_masker=RandomMasker(None, MT5TokenizerFast.from_pretrained(\"google/mt5-small\",\n",
    "                                                                                        # force_download=True,\n",
    "                                                                                        model_max_length=700, legacy=False), 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIX_FLAG = False\n",
    "LOAD_BEST = True\n",
    "TASK = \"42k_hcuch\"\n",
    "STAGE2EXP = \"mmice-mt5-small-lora-00\"\n",
    "SAVE_PATH = f\"../results/{TASK}/edits/{STAGE2EXP}/\"\n",
    "EDIT_PATH = SAVE_PATH + \"edits.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edits(path):\n",
    "    edits = pd.read_csv(path, sep=\"\\t\", lineterminator=\"\\n\").dropna()\n",
    "    edits = edits[edits['data_idx'] != 'data_idx']\n",
    "    if edits['new_pred'].dtype == np.dtype('float64'):\n",
    "        edits['new_pred'] = edits.apply(lambda row: str(int(row['new_pred']) if not np.isnan(row['new_pred']) else \"\"), axis=1)\n",
    "        edits['orig_pred'] = edits.apply(lambda row: str(int(row['orig_pred']) if not np.isnan(row['orig_pred']) else \"\"), axis=1)\n",
    "        edits['contrast_pred'] = edits.apply(lambda row: str(int(row['contrast_pred']) if not np.isnan(row['contrast_pred']) else \"\"), axis=1)\n",
    "    else:\n",
    "        edits['new_pred'].fillna(value=\"\", inplace=True)\n",
    "        edits['orig_pred'].fillna(value=\"\", inplace=True)\n",
    "        edits['contrast_pred'].fillna(value=\"\", inplace=True)\n",
    "    return edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_edits(edits):\n",
    "    \"\"\" MiCE writes all edits that are found in Stage 2, \n",
    "    but we only want to evaluate the smallest per input. \n",
    "    Calling get_sorted_e() \"\"\"\n",
    "    edits['sorted_idx'] = pd.to_numeric(edits['sorted_idx'])\n",
    "    edits['minimality'] = pd.to_numeric(edits['minimality'])\n",
    "    edits['data_idx'] = pd.to_numeric(edits['data_idx'])\n",
    "    edits['duration'] = pd.to_numeric(edits['duration'])\n",
    "    return edits[edits['sorted_idx'] == 0]\n",
    "    \n",
    "def evaluate_edits(edits):\n",
    "    temp = edits[edits['sorted_idx'] == 0]\n",
    "    minim = temp['minimality'].mean()\n",
    "    flipped = temp[temp['new_pred'].astype(str)==temp['contrast_pred'].astype(str)]\n",
    "    nunique = temp['data_idx'].nunique()\n",
    "    \n",
    "    flip_rate = len(flipped)/nunique\n",
    "    duration = temp['duration'].mean()\n",
    "    metrics = {\n",
    "        \"num_total\": nunique,\n",
    "        \"num_flipped\": len(flipped),\n",
    "        \"flip_rate\": flip_rate,\n",
    "        \"minimality\": minim,\n",
    "        #\"fluency\": temp['fluency'].mean(),\n",
    "        \"duration\": duration,\n",
    "    }\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: \\t{round(v, 3)}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_edits(row):\n",
    "    html_original, html_edited = html_highlight_diffs(row['orig_editable_seg'], row['edited_editable_seg'], nlp)\n",
    "    minim = round(row['minimality'], 3)\n",
    "    print(f\"MINIMALITY: \\t{minim}\")\n",
    "    print(\"\")\n",
    "    display(HTML(html_original))\n",
    "    display(HTML(html_edited))\n",
    "\n",
    "def display_classif_results(rows):\n",
    "    for _, row in rows.iterrows():\n",
    "        orig_contrast_prob_pred = round(row['orig_contrast_prob_pred'], 3)\n",
    "        new_contrast_prob_pred = round(row['new_contrast_prob_pred'], 3)\n",
    "        print(\"-----------------------\")\n",
    "        print(f\"ORIG LABEL: \\t{row['orig_pred']}\")\n",
    "        print(f\"CONTR LABEL: \\t{row['contrast_pred']} (Orig Pred Prob: {orig_contrast_prob_pred})\")\n",
    "        print(f\"NEW LABEL: \\t{row['new_pred']} (New Pred Prob: {new_contrast_prob_pred})\")\n",
    "        print(\"\")\n",
    "        display_edits(row)\n",
    "\n",
    "def display_race_results(rows):\n",
    "    for _, row in rows.iterrows():\n",
    "        orig_contrast_prob_pred = round(row['orig_contrast_prob_pred'], 3)\n",
    "        new_contrast_prob_pred = round(row['new_contrast_prob_pred'], 3)\n",
    "        orig_input = eval(row['orig_input'])\n",
    "        options = orig_input['options']\n",
    "        print(\"-----------------------\")\n",
    "        print(f\"QUESTION: {orig_input['question']}\")\n",
    "        print(\"\\nOPTIONS:\")\n",
    "        for opt_idx, opt in enumerate(options):\n",
    "            print(f\"  ({opt_idx}) {opt}\")\n",
    "        print(f\"\\nORIG LABEL: \\t{row['orig_pred']}\")\n",
    "        print(f\"CONTR LABEL: \\t{row['contrast_pred']} (Orig Pred Prob: {orig_contrast_prob_pred})\")\n",
    "        print(f\"NEW LABEL: \\t{row['new_pred']} (New Pred Prob: {new_contrast_prob_pred})\")\n",
    "        print(\"\")\n",
    "        display_edits(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_windows_corruption(file_path):\n",
    "    new_file_path = file_path.split(\"/\")\n",
    "    new_file_path[-1] = 'fixed_edits.csv'\n",
    "    new_file_path = \"/\".join(new_file_path)\n",
    "    with open(file_path, 'r') as f, open(new_file_path, 'w+') as new_file:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            i += 1\n",
    "            line_ = line.replace(\";\", \"\").strip()\n",
    "            if len(line_) == 0:\n",
    "                continue\n",
    "            if line_[0] == \"\\\"\":\n",
    "                line_ = line_[1:]\n",
    "            if line_[-1] == \"\\\"\":\n",
    "                line_ = line_[:-1] \n",
    "            new_file.write(line_ + \"\\n\")\n",
    "    return new_file_path\n",
    "\n",
    "\n",
    "if FIX_FLAG:\n",
    "    EDIT_PATH = fix_windows_corruption(EDIT_PATH)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_1256\\538214286.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  edits['new_pred'].fillna(value=\"\", inplace=True)\n",
      "C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_1256\\538214286.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  edits['orig_pred'].fillna(value=\"\", inplace=True)\n",
      "C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_1256\\538214286.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  edits['contrast_pred'].fillna(value=\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "edits = read_edits(EDIT_PATH) #if not LOAD_BEST else read_edits(SAVE_PATH + \"best_edits.csv\")\n",
    "edits = get_best_edits(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_idx</th>\n",
       "      <th>sorted_idx</th>\n",
       "      <th>orig_pred</th>\n",
       "      <th>new_pred</th>\n",
       "      <th>contrast_pred</th>\n",
       "      <th>orig_contrast_prob_pred</th>\n",
       "      <th>new_contrast_prob_pred</th>\n",
       "      <th>orig_input</th>\n",
       "      <th>edited_input</th>\n",
       "      <th>orig_editable_seg</th>\n",
       "      <th>edited_editable_seg</th>\n",
       "      <th>minimality</th>\n",
       "      <th>num_edit_rounds</th>\n",
       "      <th>mask_frac</th>\n",
       "      <th>duration</th>\n",
       "      <th>error\\r\\r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>nodulos</td>\n",
       "      <td>nodulos</td>\n",
       "      <td>NO nodulos</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>Árbol traqueo-bronquial principal permeable. P...</td>\n",
       "      <td>Árbol traqueo-bronquial principal permeable. P...</td>\n",
       "      <td>Árbol traqueo-bronquial principal permeable. P...</td>\n",
       "      <td>Árbol traqueo-bronquial principal permeable. P...</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.034375</td>\n",
       "      <td>24.195103</td>\n",
       "      <td>False\\r\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>nodulos</td>\n",
       "      <td>nodulos</td>\n",
       "      <td>NO nodulos</td>\n",
       "      <td>0.048486</td>\n",
       "      <td>0.955424</td>\n",
       "      <td>Árbol tráqueo-bronquial principal permeable. G...</td>\n",
       "      <td>Árbol tráqueo-bronquial principal permeable. G...</td>\n",
       "      <td>Árbol tráqueo-bronquial principal permeable. G...</td>\n",
       "      <td>Árbol tráqueo-bronquial principal permeable. G...</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.034375</td>\n",
       "      <td>17.145000</td>\n",
       "      <td>False\\r\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_idx  sorted_idx orig_pred new_pred contrast_pred  \\\n",
       "1          6           0   nodulos  nodulos    NO nodulos   \n",
       "51         4           0   nodulos  nodulos    NO nodulos   \n",
       "\n",
       "    orig_contrast_prob_pred  new_contrast_prob_pred  \\\n",
       "1                  0.009815                0.999944   \n",
       "51                 0.048486                0.955424   \n",
       "\n",
       "                                           orig_input  \\\n",
       "1   Árbol traqueo-bronquial principal permeable. P...   \n",
       "51  Árbol tráqueo-bronquial principal permeable. G...   \n",
       "\n",
       "                                         edited_input  \\\n",
       "1   Árbol traqueo-bronquial principal permeable. P...   \n",
       "51  Árbol tráqueo-bronquial principal permeable. G...   \n",
       "\n",
       "                                    orig_editable_seg  \\\n",
       "1   Árbol traqueo-bronquial principal permeable. P...   \n",
       "51  Árbol tráqueo-bronquial principal permeable. G...   \n",
       "\n",
       "                                  edited_editable_seg  minimality  \\\n",
       "1   Árbol traqueo-bronquial principal permeable. P...    0.039589   \n",
       "51  Árbol tráqueo-bronquial principal permeable. G...    0.037862   \n",
       "\n",
       "    num_edit_rounds  mask_frac   duration  error\\r\\r  \n",
       "1               1.0   0.034375  24.195103  False\\r\\r  \n",
       "51              1.0   0.034375  17.145000  False\\r\\r  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "edited_results = perplexity.compute(predictions=edits['edited_input'].tolist()[:], model_id='facebook/xglm-1.7B', batch_size=1)\n",
    "orig_results = perplexity.compute(predictions=edits['orig_input'].tolist()[:], model_id='facebook/xglm-1.7B', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edited_results['mean_perplexity'])\n",
    "print(orig_results['mean_perplexity'])\n",
    "\n",
    "\n",
    "edits['edit_perplexity'] = edited_results['perplexities']\n",
    "edits['orig_perplexity'] = orig_results['perplexities']\n",
    "edits.to_parquet(SAVE_PATH + \"best_edits.parquet.gzip\",\n",
    "                 compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm.pandas(desc='original sequence loss!')\n",
    "# a = edits[\"orig_editable_seg\"].progress_apply(lambda x: eval.score_fluency(x, 2))\n",
    "\n",
    "# tqdm.pandas(desc='edited sequence loss!')\n",
    "# b = edits[\"edited_editable_seg\"].progress_apply(lambda x: eval.score_fluency(x) if isinstance(x, str) else 0)\n",
    "\n",
    "# edits['fluency'] =  b/a\n",
    "# edits.to_csv(SAVE_PATH + \"best_edits.csv\", sep=\"\\t\", lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_total: \t2\n",
      "num_flipped: \t0\n",
      "flip_rate: \t0.0\n",
      "minimality: \t0.039\n",
      "duration: \t20.67\n"
     ]
    }
   ],
   "source": [
    "#edits = read_edits(SAVE_PATH + \"best_edits.csv\")\n",
    "#edits = get_best_edits(edits)\n",
    "metrics = evaluate_edits(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "ORIG LABEL: \tnodulos\n",
      "CONTR LABEL: \tNO nodulos (Orig Pred Prob: 0.01)\n",
      "NEW LABEL: \tnodulos (New Pred Prob: 1.0)\n",
      "\n",
      "MINIMALITY: \t0.04\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Árbol traqueo-bronquial principal permeable. Pulmones adecuadamente insuflados, <b>no</b> se observan focos condensación ni opacidades en vidrio esmerilado. Nódulos pulmonares menores de 5 mm, en el lóbulo superior derecho y <b>lóbulo</b> inferior derecho, <b>algunos</b> calcificados de aspecto benigno y otros inespecíficos. Superficies <b>pleurales</b> y pericardio sin alteraciones. Corazón de tamaño normal. Calcificaciones en el plano valvular aórtico. Grandes vasos mediastínicos de trayecto y calibre normales. Mínima ateromatosis cálcica del arco aórtico y aorta descendente. No se observa <b>linfonodos</b> mediastínicos mayores de 10 mm en el eje menor. Esqueleto torácico sin lesiones de aspecto agresivo."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Árbol traqueo-bronquial principal permeable. Pulmones adecuadamente insuflados, se observan focos condensación ni opacidades en vidrio esmerilado. Nódulos pulmonares menores de 5 mm, en el <b> </b>lóbulo superior derecho y <b> </b><b>el</b> <b>lobo</b> <b>lbulo</b> inferior derecho, <b> </b><b>segmentos</b> calcificados de aspecto benigno y otros inespecíficos. Superficies <b> </b><b>fracales</b> y pericardio sin alteraciones. Corazón de tamaño normal. Calcificaciones en el plano valvular aórtico. Grandes vasos mediastínicos de trayecto y calibre normales. Mínima ateromatosis cálcica del arco aórtico y aorta descendente. No se observa <b>linciofonodos</b> mediastínicos mayores de 10 mm en el eje menor. Esqueleto torácico sin lesiones de aspecto agresivo."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_rows = edits.sample(1)\n",
    "display_classif_results(random_rows)\n",
    "# display_race_results(random_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
