{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\multilingual_mice\\.env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mmice.utils import html_highlight_diffs\n",
    "from mmice.edit_finder import EditEvaluator\n",
    "from mmice.maskers.random_masker import RandomMasker\n",
    "from transformers import MT5TokenizerFast\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "eval = EditEvaluator(fluency_model_name=\"google/mt5-small\",\n",
    "                     fluency_masker=RandomMasker(None, MT5TokenizerFast.from_pretrained(\"google/mt5-small\", model_max_length=700, legacy=False), 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"chileanhate\"\n",
    "STAGE2EXP = \"mmice-test-editor-final\"\n",
    "SAVE_PATH = f\"../results/{TASK}/edits/{STAGE2EXP}/\"\n",
    "EDIT_PATH = SAVE_PATH + \"edits.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edits(path):\n",
    "    edits = pd.read_csv(path, sep=\"\\t\", lineterminator=\"\\n\")\n",
    "    if edits['new_pred'].dtype == np.dtype('float64'):\n",
    "        edits['new_pred'] = edits.apply(lambda row: str(int(row['new_pred']) if not np.isnan(row['new_pred']) else \"\"), axis=1)\n",
    "        edits['orig_pred'] = edits.apply(lambda row: str(int(row['orig_pred']) if not np.isnan(row['orig_pred']) else \"\"), axis=1)\n",
    "        edits['contrast_pred'] = edits.apply(lambda row: str(int(row['contrast_pred']) if not np.isnan(row['contrast_pred']) else \"\"), axis=1)\n",
    "    else:\n",
    "        edits['new_pred'].fillna(value=\"\", inplace=True)\n",
    "        edits['orig_pred'].fillna(value=\"\", inplace=True)\n",
    "        edits['contrast_pred'].fillna(value=\"\", inplace=True)\n",
    "    return edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_edits(edits):\n",
    "    \"\"\" MiCE writes all edits that are found in Stage 2, \n",
    "    but we only want to evaluate the smallest per input. \n",
    "    Calling get_sorted_e() \"\"\"\n",
    "    return edits[edits['sorted_idx'] == 0]\n",
    "    \n",
    "def evaluate_edits(edits):\n",
    "    temp = edits[edits['sorted_idx'] == 0]\n",
    "    minim = temp['minimality'].mean()\n",
    "    flipped = temp[temp['new_pred'].astype(str)==temp['contrast_pred'].astype(str)]\n",
    "    nunique = temp['data_idx'].nunique()\n",
    "    flip_rate = len(flipped)/nunique\n",
    "    duration = temp['duration'].mean()\n",
    "    metrics = {\n",
    "        \"num_total\": nunique,\n",
    "        \"num_flipped\": len(flipped),\n",
    "        \"flip_rate\": flip_rate,\n",
    "        \"minimality\": minim,\n",
    "        \"fluency\": temp['fluency'].mean(),\n",
    "        \"duration\": duration,\n",
    "    }\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: \\t{round(v, 3)}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_edits(row):\n",
    "    html_original, html_edited = html_highlight_diffs(row['orig_editable_seg'], row['edited_editable_seg'], nlp)\n",
    "    minim = round(row['minimality'], 3)\n",
    "    print(f\"MINIMALITY: \\t{minim}\")\n",
    "    print(\"\")\n",
    "    display(HTML(html_original))\n",
    "    display(HTML(html_edited))\n",
    "\n",
    "def display_classif_results(rows):\n",
    "    for _, row in rows.iterrows():\n",
    "        orig_contrast_prob_pred = round(row['orig_contrast_prob_pred'], 3)\n",
    "        new_contrast_prob_pred = round(row['new_contrast_prob_pred'], 3)\n",
    "        print(\"-----------------------\")\n",
    "        print(f\"ORIG LABEL: \\t{row['orig_pred']}\")\n",
    "        print(f\"CONTR LABEL: \\t{row['contrast_pred']} (Orig Pred Prob: {orig_contrast_prob_pred})\")\n",
    "        print(f\"NEW LABEL: \\t{row['new_pred']} (New Pred Prob: {new_contrast_prob_pred})\")\n",
    "        print(\"\")\n",
    "        display_edits(row)\n",
    "\n",
    "def display_race_results(rows):\n",
    "    for _, row in rows.iterrows():\n",
    "        orig_contrast_prob_pred = round(row['orig_contrast_prob_pred'], 3)\n",
    "        new_contrast_prob_pred = round(row['new_contrast_prob_pred'], 3)\n",
    "        orig_input = eval(row['orig_input'])\n",
    "        options = orig_input['options']\n",
    "        print(\"-----------------------\")\n",
    "        print(f\"QUESTION: {orig_input['question']}\")\n",
    "        print(\"\\nOPTIONS:\")\n",
    "        for opt_idx, opt in enumerate(options):\n",
    "            print(f\"  ({opt_idx}) {opt}\")\n",
    "        print(f\"\\nORIG LABEL: \\t{row['orig_pred']}\")\n",
    "        print(f\"CONTR LABEL: \\t{row['contrast_pred']} (Orig Pred Prob: {orig_contrast_prob_pred})\")\n",
    "        print(f\"NEW LABEL: \\t{row['new_pred']} (New Pred Prob: {new_contrast_prob_pred})\")\n",
    "        print(\"\")\n",
    "        display_edits(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = read_edits(EDIT_PATH)\n",
    "edits = get_best_edits(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc='original sequence loss!')\n",
    "a = edits[\"orig_editable_seg\"].progress_apply(lambda x: eval.score_fluency(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc='edited sequence loss!')\n",
    "b = edits[\"edited_editable_seg\"].progress_apply(lambda x: eval.score_fluency(x) if isinstance(x, str) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits['fluency'] =  b/a\n",
    "edits.to_csv(SAVE_PATH + \"best_edits.csv\", sep=\"\\t\", lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = read_edits(SAVE_PATH + \"best_edits.csv\")\n",
    "edits = get_best_edits(edits)\n",
    "metrics = evaluate_edits(edits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "ORIG LABEL: \tLABEL_0\n",
      "CONTR LABEL: \tLABEL_1 (Orig Pred Prob: 0.0)\n",
      "NEW LABEL: \tLABEL_1 (New Pred Prob: 0.987)\n",
      "\n",
      "MINIMALITY: \t0.414\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "por <b>todo</b> <b>peleamos</b> con el <b>negro</b> ni un día <b>bien</b> <b>podemos</b> <b>estar</b> <b>lpm</b> jajaja"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "por <b>a</b> <b>amos</b> con el <b> </b><b>mi</b> <b> </b>ni un día <b> </b><b>la</b> <b>weon</b> <b>emos</b> <b> </b><b>q</b> <b>  </b><b>q</b> <b> </b>jajaja"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_rows = edits.sample(1)\n",
    "display_classif_results(random_rows)\n",
    "# display_race_results(random_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
